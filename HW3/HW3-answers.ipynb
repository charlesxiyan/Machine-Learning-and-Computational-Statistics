{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02eb922",
   "metadata": {},
   "source": [
    "### Homework 3\n",
    "##### Charles Yan, xy2985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499298b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe82aba",
   "metadata": {},
   "source": [
    "#### Q1 The classical linear regression model\n",
    "\n",
    "(a) \n",
    "Let $\\bm{A} = \\bm{X}^{\\top}\\bm{X}$. It is easy to show $\\bm{A}$ is symmetric.\n",
    "\n",
    "Since $\\bm{AA}^{-1} = \\bm{I}$, taking transpose on both sides:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "(\\bm{A}^{-1})^{\\top}\\bm{A}^{\\top} &= \\bm{I} \\\\\n",
    "\\Leftrightarrow \\quad (\\bm{A}^{-1})^{\\top}\\bm{A} &= \\bm{I} \\quad (\\text{since } \\bm{A}^{\\top} = \\bm{A}) \\\\\n",
    "\\Leftrightarrow \\quad (\\bm{A}^{-1})^{\\top} &= \\bm{A}^{-1}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Thus, $\\bm{A}^{-1}$ is symmetric.\n",
    "\n",
    "$\\bm{P}$ is an orthogonal projection, if for any vector $\\bm{y}\\in\\mathbb{R}^{n}$,\n",
    "$$(\\bm{I} - \\bm{P})\\bm{y} = (\\bm{y} - \\bm{P}\\bm{y})$$\n",
    "is orthogonal to $\\bm{Py}$, i.e. the inner product being zero.\n",
    "\n",
    "Now, we can see\n",
    "$$\\begin{aligned}(Py)^{\\top}(I-P)y &= y^{\\top}P^\\top(I-P)y\\\\ &= y^\\top(P^\\top - P^\\top P)y\\\\ &=y^\\top(P - PP)y \\\\ &=y^\\top(P - P)y \\\\ &= 0\\end{aligned}$$\n",
    "since $P$ is an symmetric and idempotent matrix, as shown in Homework 2 - 2(a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1b9ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Frobenius Norm of P - P is 4.09e-16\n",
      "=============================================\n",
      "Frobenius Norm of P^2 - P is 6.87e-16\n",
      "=============================================\n",
      "Frobenius Norm of X^T(y - Py) is 3.23e-15\n",
      "=============================================\n",
      "Rank of P is 5.00e+00 v.s. p = 5.\n"
     ]
    }
   ],
   "source": [
    "### (b)\n",
    "np.random.seed(99)\n",
    "\n",
    "n, p = 50, 5\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "y = np.random.normal(0, 1, (n, 1))\n",
    "P = X @ np.linalg.solve(X.T @ X, X.T)\n",
    "\n",
    "### Tets\n",
    "print('=' * 45)\n",
    "print(f'Frobenius Norm of P - P is {np.linalg.norm(P - P.T):.2e}')\n",
    "print('=' * 45)\n",
    "print(f'Frobenius Norm of P^2 - P is {np.linalg.norm(P @ P - P):.2e}')\n",
    "print('=' * 45)\n",
    "print(f'Frobenius Norm of X^T(y - Py) is {np.linalg.norm(X.T @ (y - P @ y)):.2e}')\n",
    "print('=' * 45)\n",
    "print(f'Rank of P is {np.linalg.matrix_rank(P):.2e} v.s. p = {p}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f0cd6",
   "metadata": {},
   "source": [
    "##### (c) No, if the model is under the assumptions of OLS.\n",
    "Under assumptions of OLS model,\n",
    "$$\\begin{aligned}\n",
    "\\sum_{i = 1}^{n}(y_{i} - \\bar{y})^2 &= \\sum_{i = 1}^{n}(y_{i} - \\hat{y}_{i} + \\hat{y}_{i} - \\bar{y})^2\\\\\n",
    "&= \\sum_{i = 1}^{n}(y_{i} - \\hat{y}_{i})^2 + \\sum_{i = 1}^{n}(\\hat{y}_{i} - \\bar{y})^2 + 2\\sum_{i = 1}^{n}(y_{i} - \\hat{y}_{i})(\\hat{y}_{i} - \\bar{y}),\n",
    "\\end{aligned}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\begin{aligned}\\sum_{i = 1}^{n}(y_{i} - \\hat{y}_{i})(\\hat{y}_{i} - \\bar{y})\n",
    "&=\\sum_{i = 1}^{n}(y_{i} - x_{i}^{\\top}\\beta)(x_{i}^{\\top}\\beta - \\bar{y})\\\\\n",
    "&=\\beta^{\\top}\\sum_{i = 1}^{n}(y_{i} - x_{i}^{\\top}\\beta)x_i - \\bar{y}\\sum_{i = 1}^{n}(y_{i} - x_{i}^{\\top}\\beta)\\\\\n",
    "&=0\\end{aligned}\n",
    "$$\n",
    "\n",
    "based on OLS conditions.\n",
    "\n",
    "Hence, by the decomposition above,\n",
    "$$\\begin{aligned}R^2 &= 1 - \\dfrac{\\sum_{i = 1}^{n}(y_{i} - \\hat{y}_{i})^2}{\\sum_{i = 1}^{n}(y_{i} - \\bar{y})^2}\\\\\n",
    "&=\\dfrac{\\sum_{i = 1}^{n}(y_{i} - \\bar{y})^2 - \\sum_{i = 1}^{n}(y_{i} - \\hat{y}_{i})^2}{\\sum_{i = 1}^{n}(y_{i} - \\bar{y})^2}\\\\\n",
    "&=\\dfrac{\\sum_{i = 1}^{n}(\\hat{y}_{i} - \\bar{y})^2}{\\sum_{i = 1}^{n}(y_{i} - \\bar{y})^2}\\\\\n",
    "&\\in [0,1].\n",
    "\\end{aligned}$$\n",
    "\n",
    "It should be noted that if the data is not approximated by linear regression model, say a non-linear model, $R^2$ could be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c21d5",
   "metadata": {},
   "source": [
    "##### (d) \n",
    "Since $\\nabla_{\\beta}(u^{\\top}v) = (\\nabla_{\\beta}u)v + (\\nabla_{\\beta}v)u$,\n",
    "we let $u = \\beta$, $v = A\\beta$,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_{\\beta}(u^{\\top}v) &= \\dfrac{\\partial\\beta}{\\partial\\beta}A\\beta + \\dfrac{\\partial A\\beta}{\\partial\\beta}\\beta\\\\\n",
    "&=IA\\beta + A^{\\top}\\beta\\\\\n",
    "&=(A + A^{\\top})\\beta\\\\\n",
    "&=2A\\beta.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc1e5d",
   "metadata": {},
   "source": [
    "##### (e)\n",
    "(1) The fixed regressors are treated as fixed, known constants. They are not random variables. The values of X are considered predetermined and the inference is conditional on the observed values of X. The linear model is:\n",
    "$$y = X\\beta + \\varepsilon,$$\n",
    "X is fixed and $\\varepsilon$ is random, with an assumption $E[\\varepsilon] = 0$.\n",
    "\n",
    "(2) The random regressors are random variables, drawn from some joint distribution with the dependent variable, i.e. X and y being both random. For the same model $y = X\\beta + \\varepsilon$, we need an assumption $E[\\varepsilon\\vert X] = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b7731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Emp Var - Real Var: 1.2257e-04\n",
      "=============================================\n",
      "=============================================\n",
      "Variance of each beta components (i): [0.0042042  0.00561618 0.00591289]\n",
      "=============================================\n",
      "Variance of each beta components (ii): [0.00517303 0.00508562 0.0051467 ]\n"
     ]
    }
   ],
   "source": [
    "### (f)\n",
    "np.random.seed(99)\n",
    "n, p, B = 200, 3, 5000\n",
    "beta = np.array([0.4, 0.5, 0.6])\n",
    "std = 1\n",
    "\n",
    "### (i)\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "A = np.linalg.inv(X.T @ X)\n",
    "varBetaHat = std ** 2 * A\n",
    "\n",
    "betaHatI = np.zeros((B, p))\n",
    "for b in range(B):\n",
    "    y = X @ beta + np.random.normal(0, 1, n) * std\n",
    "    betaHatI[b] = A @ X.T @ y\n",
    "empVarI = np.cov(betaHatI, rowvar=False)\n",
    "print('=' * 45)\n",
    "print(f'Emp Var - Real Var: {np.linalg.norm(empVarI - varBetaHat):.4e}')\n",
    "print('=' * 45)\n",
    "\n",
    "### (ii)\n",
    "betaHatII = np.zeros((B, p))\n",
    "for b in range(B):\n",
    "    XRand = np.random.normal(0, 1, (n, p))\n",
    "    y = XRand @ beta + np.random.normal(0, 1, n) * std\n",
    "    betaHatII[b] = np.linalg.solve(XRand.T @ XRand, XRand.T @ y)\n",
    "empVarII = np.cov(betaHatII, rowvar=False)\n",
    "print('=' * 45)\n",
    "print(f'Variance of each beta components (i): {np.diag(empVarI)}')\n",
    "print('=' * 45)\n",
    "print(f'Variance of each beta components (ii): {np.diag(empVarII)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac638f",
   "metadata": {},
   "source": [
    "Summary:\n",
    "Conditioning on X - the variability of $\\beta$ only from $\\varepsilon$.\n",
    "\n",
    "Averaging on X - the variability of $\\beta$ from both $\\varepsilon$ and X. As n grows larger, $X^{\\top}X$ converges to a constant matrix, and the outcome gradually coincides with the result conditioning on X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24cacd",
   "metadata": {},
   "source": [
    "##### Q2 Finite-sample properties of OLS\n",
    "\n",
    "(a) Assumptions\n",
    "(i) Linearity $\\bm{Y} = \\bm{X\\beta} + \\bm{\\epsilon}$.\n",
    "\n",
    "(ii) Strict Exogeneity $E[\\bm{\\epsilon\\vert\\bm{X}}] = \\bm{0}$.\n",
    "\n",
    "(iii) No Multicolinearity $P(Rank(\\bm{X}) = p) = 1$.\n",
    "\n",
    "(iv) Spherical Errors $Var(\\bm{\\epsilon}\\vert\\bm{X}) = \\sigma^{2}\\bm{I}_{n}$.\n",
    "\n",
    "Based on the result in Homework 2 - 2(a), we know $M = I_{n} -P$ is symmetric and idempotent.\n",
    "\n",
    "The result in Homework 2 - 3(a) shows that\n",
    "$$\\begin{aligned}\n",
    "s^2 = & \\dfrac{\\varepsilon^\\top\\varepsilon}{n-p}\\\\\n",
    "=&\\dfrac{(y-\\hat{y})^\\top(y-\\hat{y})}{n - p}\\\\\n",
    "=&\\dfrac{\\varepsilon^\\top M\\varepsilon}{n-p}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Thus, we have\n",
    "$$\\begin{aligned}\n",
    "E[s^2\\vert X] =& \\dfrac{1}{n-p}E[\\varepsilon^\\top M\\varepsilon\\vert X]\\\\\n",
    "=& \\dfrac{1}{n-p}E[Tr(\\varepsilon^\\top M\\varepsilon)\\vert X]\\\\\n",
    "=& \\dfrac{1}{n-p}E[Tr(M\\varepsilon^\\top\\varepsilon)\\vert X]\\\\\n",
    "=& \\dfrac{1}{n-p}Tr(ME[\\varepsilon^\\top\\varepsilon\\vert X])\\\\\n",
    "=& \\dfrac{1}{n-p}Tr(M(Var(\\varepsilon\\vert X) + E[\\varepsilon\\vert X]E[\\varepsilon\\vert X]^\\top))\\\\\n",
    "=& \\dfrac{1}{n-p}Tr(M\\sigma^2 I_n)\\\\\n",
    "=& \\dfrac{\\sigma^2}{n-p}Tr(I_n - P)\\\\\n",
    "=& \\dfrac{\\sigma^2}{n-p}[Tr(I_n) - Tr(P)]\\\\\n",
    "=& \\dfrac{\\sigma^2}{n-p}[n - Tr(X(X^\\top X)^{-1}X^\\top)]\\\\\n",
    "=& \\dfrac{\\sigma^2}{n-p}[n - Tr((X^\\top X)^{-1}X^\\top X)]\\\\\n",
    "=& \\dfrac{\\sigma^2}{n-p}(n - p)\\\\\n",
    "=& \\sigma^2\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a401e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Empirical mean of s2Unbias: 1.9906e+00, Real Variance: 2.0000e+00\n",
      "=============================================\n",
      "Empirical mean of s2Bias: 1.8662e+00, Real Variance: 2.0000e+00\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "### (b)\n",
    "np.random.seed(99)\n",
    "n, p, var, B = 80, 5, 2, 5000\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "beta = np.random.normal(0, 1, p)\n",
    "\n",
    "s2Bias = np.zeros(B)\n",
    "s2Unbias = np.zeros(B)\n",
    "\n",
    "for b in range(B):\n",
    "    eps = np.random.normal(0, np.sqrt(var), n)\n",
    "    y = X @ beta + eps\n",
    "    betaHat = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "    r = y - X @ betaHat\n",
    "    s2Unbias[b] = r @ r / (n - p)\n",
    "    s2Bias[b] = r @ r / n\n",
    "\n",
    "print('=' * 45)\n",
    "print(f'Empirical mean of s2Unbias: {s2Unbias.mean():.4e}, Real Variance: {var:.4e}')\n",
    "print('=' * 45)\n",
    "print(f'Empirical mean of s2Bias: {s2Bias.mean():.4e}, Real Variance: {var:.4e}')\n",
    "print('=' * 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b3ac6",
   "metadata": {},
   "source": [
    "##### As is shown in (a) that $\\dfrac{\\bm{r}^{\\top}\\bm{r}}{n - p}$ is an unbiased estimator of $\\bm{\\sigma}^{2}$. An adjustment on the denominator can make the statistic biased, say $\\dfrac{\\bm{r}^{\\top}\\bm{r}}{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f56a0",
   "metadata": {},
   "source": [
    "##### (c)\n",
    "(i) Linearity $\\bm{Y} = \\bm{X\\beta} + \\bm{\\epsilon}$.\n",
    "\n",
    "(ii) Strict Exogeneity $E[\\bm{\\epsilon\\vert\\bm{X}}] = \\bm{0}$.\n",
    "\n",
    "(iii) No Multicolinearity $P(Rank(\\bm{X}) = p) = 1$.\n",
    "\n",
    "(iv) Spherical Errors $Var(\\bm{\\epsilon}\\vert\\bm{X}) = \\sigma^{2}\\bm{I}_{n}$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\hat{\\bm{\\beta}}\\vert \\bm{X} &= (\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top\\bm{y} \\vert \\bm{X}\\\\\n",
    "&= (\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top(\\bm{X}\\bm{\\beta} + \\bm{\\varepsilon}) \\vert\\bm{X}\\\\\n",
    "&= \\bm{\\beta} + (\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top\\bm{\\varepsilon}\\vert \\bm{X}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Let $\\bm{A} = (\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top$, then:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{Var}(\\hat{\\bm{\\beta}} \\mid \\bm{X}) &=\\text{Var}(\\bm{\\beta} + \\bm{A}\\bm{\\varepsilon} \\mid \\bm{X})\\\\\n",
    "&= \\text{Var}(\\bm{A}\\bm{\\varepsilon} \\mid \\bm{X}) \\\\\n",
    "&= \\bm{A} \\cdot \\text{Var}(\\bm{\\varepsilon} \\mid \\bm{X}) \\cdot \\bm{A}^\\top \\\\\n",
    "&= (\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top \\cdot \\sigma^2\\bm{I} \\cdot \\bm{X}(\\bm{X}^\\top\\bm{X})^{-1} \\\\\n",
    "&= \\sigma^2(\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top\\bm{X}(\\bm{X}^\\top\\bm{X})^{-1} \\\\\n",
    "&= \\sigma^2(\\bm{X}^\\top\\bm{X})^{-1} \\quad \\blacksquare\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27b663",
   "metadata": {},
   "source": [
    "##### (d)\n",
    "As is shown in Homework 2 - 3(a), $\\bm{r} = \\bm{M\\varepsilon} = (\\bm{I} - \\bm{P})\\bm{\\varepsilon}$.\n",
    "\n",
    "Since $\\hat{\\bm{\\beta}} = \\bm{\\beta} + (\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top\\bm{\\varepsilon}$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{Cov}(\\hat{\\bm{\\beta}}, \\bm{r} \\mid \\bm{X}) &= \\text{Cov}\\bigl(\\bm{\\beta} + (\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top\\bm{\\varepsilon},\\; (\\bm{I}-\\bm{P})\\bm{\\varepsilon} \\mid \\bm{X}\\bigr) \\\\\n",
    "&= \\text{E}[(\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top\\bm{\\varepsilon\\varepsilon}^{\\top}(\\bm{I} - \\bm{P})^\\top\\vert\\bm{X}]\\\\\n",
    "&= (\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top \\cdot \\text{Var}(\\bm{\\varepsilon} \\mid \\bm{X}) \\cdot (\\bm{I}-\\bm{P})^\\top \\\\\n",
    "&= (\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top \\cdot \\sigma^2\\bm{I} \\cdot (\\bm{I}-\\bm{P}) \\\\\n",
    "&= \\sigma^2 (\\bm{X}^\\top\\bm{X})^{-1} \\bm{X}^\\top(\\bm{I}-\\bm{P})\n",
    "\\end{aligned}$$\n",
    "\n",
    "Note that:\n",
    "\n",
    "$$\\bm{X}^\\top(\\bm{I} - \\bm{P}) = \\bm{X}^\\top - \\bm{X}^\\top\\bm{P} = \\bm{X}^\\top - \\bm{X}^\\top\\bm{X}(\\bm{X}^\\top\\bm{X})^{-1}\\bm{X}^\\top = \\bm{X}^\\top - \\bm{X}^\\top = \\bm{0}$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\text{Cov}(\\hat{\\bm{\\beta}}, \\bm{r} \\mid \\bm{X}) = \\sigma^2 (\\bm{X}^\\top\\bm{X})^{-1} \\cdot \\bm{0} = \\bm{0} \\quad \\blacksquare$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23811a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "B = 100: Frobenius Norm of CHat is 4.4358e-01\n",
      "=============================================\n",
      "=============================================\n",
      "B = 500: Frobenius Norm of CHat is 2.1559e-01\n",
      "=============================================\n",
      "=============================================\n",
      "B = 1000: Frobenius Norm of CHat is 1.4592e-01\n",
      "=============================================\n",
      "=============================================\n",
      "B = 5000: Frobenius Norm of CHat is 6.0446e-02\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "### (e)\n",
    "### Note: n, p, var, X, beta are the same as (b)\n",
    "np.random.seed(99)\n",
    "BList = [100, 500, 1000, 5000]\n",
    "\n",
    "for B in BList:\n",
    "    betaHats = np.zeros((B, p))\n",
    "    rs = np.zeros((B, n))\n",
    "\n",
    "    for b in range(B):\n",
    "        eps = np.random.normal(0, np.sqrt(var), n)\n",
    "        y = X @ beta + eps\n",
    "        betaHats[b] = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "        rs[b] = y - X @ betaHats[b]\n",
    "\n",
    "    betaHatMean = betaHats.mean(axis=0)\n",
    "    rMean = rs.mean(axis=0)\n",
    "    CHat = (betaHats - betaHatMean).T @ (rs - rMean) / (B - 1)\n",
    "\n",
    "    print('=' * 45)\n",
    "    print(f'B = {B}: Frobenius Norm of CHat is {np.linalg.norm(CHat):.4e}')\n",
    "    print('=' * 45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a9199",
   "metadata": {},
   "source": [
    "##### As shown above in the numerical outcome, the Frobenius norm of $\\hat{\\bm{C}}$ is shrinking as B goes larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f624759",
   "metadata": {},
   "source": [
    "##### Q3 Hypothesis Testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed3b3e",
   "metadata": {},
   "source": [
    "(a)\n",
    "\n",
    "(i) In the Linear Model $y = X\\beta + \\varepsilon$, we want to test the null hypothesis\n",
    "\n",
    "$$H_0: R\\beta = d$$\n",
    "\n",
    "where $R \\in \\mathbb{R}^{d \\times p}$ with rank d, and $d \\in \\mathbb{R}^d$ are known.\n",
    "\n",
    "(ii) We define F-statistic\n",
    "\n",
    "$$F = \\frac{(R\\hat{\\beta} - d)^{\\top}[R(X^{\\top}X)^{-1}R^{\\top}]^{-1}(R\\hat{\\beta} - d) / d}{s^2}$$\n",
    "\n",
    "where $s^2 = r^{\\top}r/(n-p)$. Under the classical linear regression assumptions (I) - (V) (as shown in Q2(a)), the F-statistic follows an F-distribution with (d, n - p) degrees of freedom under the null hypothesis, that is, $F|X, H_0 \\sim F(d, n-p)$. \n",
    "\n",
    "Given a confidence level $\\alpha\\in [0, 1]$, if $F > F_{d,n-p,\\alpha}$, we reject $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec2e70",
   "metadata": {},
   "source": [
    "(b)\n",
    "\n",
    "Under $H_0: \\beta_2 = \\cdots = \\beta_p = 0$. We construct such a matrix $\\bm{R}$\n",
    "\n",
    "$$\\bm{R} = [\\bm{0}_{p-1} \\mid \\bm{I}_{p-1}] \\in \\mathbb{R}^{(p-1) \\times p},$$\n",
    "\n",
    "and $\\bm{d} = \\bm{0}_{p-1} \\in \\mathbb{R}^{p-1}$.\n",
    "\n",
    "Thus, for $\\bm{\\beta} = [\\beta_1, \\beta_2, \\cdots, \\beta_p]$, \n",
    "\n",
    "$H_0: \\beta_2 = \\cdots = \\beta_p = 0$ is equivalent to $H_0: \\bm{R}\\bm{\\beta} = \\bm{d}$\n",
    "\n",
    "Based on the theory in (a):\n",
    "\n",
    "$$F = \\frac{(\\bm{R}\\hat{\\bm{\\beta}})^{\\top}[\\bm{R}(\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{R}^{\\top}]^{-1}(\\bm{R}\\hat{\\bm{\\beta}}) / (p-1)}{s^2} \\sim F(p-1, n-p)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d062770",
   "metadata": {},
   "source": [
    "(c)\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "(i) Linearity $\\bm{Y} = \\bm{X\\beta} + \\bm{\\epsilon}$.\n",
    "\n",
    "(ii) Strict Exogeneity $E[\\bm{\\epsilon\\vert\\bm{X}}] = \\bm{0}$.\n",
    "\n",
    "(iii) No Multicolinearity $P(Rank(\\bm{X}) = p) = 1$.\n",
    "\n",
    "(iv) Spherical Errors $Var(\\bm{\\epsilon}\\vert\\bm{X}) = \\sigma^{2}\\bm{I}_{n}$.\n",
    "\n",
    "(v) Normality $\\bm{\\epsilon}\\vert\\bm{X} \\sim N(\\bm{0}, \\sigma^{2}\\bm{I}_{n})$.\n",
    "\n",
    "Proof:\n",
    "\n",
    "Since $\\bm{\\epsilon}\\vert\\bm{X}\\sim N(\\bm{0}, \\sigma^2\\bm{I}_n)$ based on assumption (v), we have\n",
    "\n",
    "$$\\begin{aligned}\\hat{\\bm{\\beta}}\\vert\\bm{X} &= (\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top}\\bm{y}\\\\ &= \\bm{\\beta} + (\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top}\\bm{\\epsilon}\\\\ &\\sim N(\\bm{\\beta}, \\sigma^2(\\bm{X}^{\\top}\\bm{X})^{-1})\\end{aligned}$$\n",
    "\n",
    "Under $H_0$, we have $\\bm{R\\beta} - \\bm{d} = \\bm{0}$.\n",
    "\n",
    "Hence, we have \n",
    "\n",
    "$$\\begin{aligned}(\\bm{R}\\hat{\\bm{\\beta}} - \\bm{d})\\vert\\bm{X} \\sim N(\\bm{0}, \\sigma^2 \\bm{R}(\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{R}^{\\top})\\end{aligned}$$\n",
    "\n",
    "and further have\n",
    "\n",
    "$$[\\bm{R}(\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{R}^{\\top}]^{-1/2}\\begin{aligned}(\\bm{R}\\hat{\\bm{\\beta}} - \\bm{d})/\\sigma\\vert\\bm{X} \\sim N(\\bm{0}, \\bm{I}_{d})\\end{aligned}$$\n",
    "\n",
    "Next,\n",
    "\n",
    "$$\\dfrac{(\\bm{R}\\hat{\\bm{\\beta}} - \\bm{d})^\\top[\\bm{R}(\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{R}^{\\top}]^{-1}(\\bm{R}\\hat{\\bm{\\beta}} - \\bm{d})}{\\sigma^2}\\sim \\chi^2_{d}.$$\n",
    "\n",
    "Recall $\\bm{M} = \\bm{I}_{n} - \\bm{X}(\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top}$ is a symmetric and idempotent matrix.\n",
    "\n",
    "Thus, the eigen values of $\\bm{M}$ only have either 1 and 0. By decomposition, we can find an orthogonal matrix \\bm{Q} such that\n",
    "\n",
    "$$M = \\bm{Q}\\bm{\\Lambda}\\bm{Q}^{\\top},$$\n",
    "\n",
    "where $\\bm{\\Lambda} = diag(1, 1, \\cdots, 1, 0, \\cdots, 0)$, with n - p 1s and p 0s.\n",
    "\n",
    "Based on Assumption (v), $$\\bm{\\epsilon}/\\sigma\\vert\\bm{X} \\sim N(\\bm{0}, \\bm{I}_{n})$$\n",
    "\n",
    "Let $\\bm{w} = Q^{\\top}\\epsilon/\\sigma\\sim N(\\bm{0}, \\bm{Q}^{\\top}\\bm{IQ}) = N(\\bm{0}, \\bm{I})$.\n",
    "\n",
    "Hence,\n",
    "\n",
    "$\\begin{aligned}\\dfrac{\\bm{\\epsilon}^{\\top}\\bm{M\\epsilon}}{\\sigma^2} = \\bm{w}^{\\top}\\bm{\\Lambda}\\bm{w} = w_{1}^{2} + w_{2}^{2} + \\cdots + w_{n-p}^{2}\\sim \\chi^{2}_{n-p}.\\end{aligned}$\n",
    "\n",
    "Now, an independece between $\\bm{M\\varepsilon}$ and $\\bm{R}\\hat{\\bm{\\beta}} - \\bm{d}$ is needed. Equivalently, we only need\n",
    "\n",
    "$$\\text{Cov}(\\bm{M\\varepsilon}, \\hat{\\bm{\\beta}}\\vert\\bm{X}) = \\text{Cov}(\\bm{r}, \\hat{\\bm{\\beta}}\\vert\\bm{X}) = 0,$$\n",
    "\n",
    "as proved in Q2(d).\n",
    "\n",
    "Finally, following the instruction to construct the F-ratio,\n",
    "\n",
    "$$\\dfrac{((\\bm{R}\\hat{\\bm{\\beta}} - \\bm{d})^\\top[\\bm{R}(\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{R}^{\\top}]^{-1}(\\bm{R}\\hat{\\bm{\\beta}} - \\bm{d})/\\sigma^2)/d}{(\\bm{\\epsilon}^{\\top}\\bm{M\\epsilon}/\\sigma^2)/(n-p)}\\sim F_{(d, n-p)},$$\n",
    "\n",
    "i.e. \n",
    "$$\\dfrac{(\\bm{R}\\hat{\\bm{\\beta}} - \\bm{d})^\\top[\\bm{R}(\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{R}^{\\top}]^{-1}(\\bm{R}\\hat{\\bm{\\beta}} - \\bm{d})/d}{\\bm{\\epsilon}^{\\top}\\bm{M\\epsilon}/(n-p)}\\sim F_{(d, n-p)}.$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
