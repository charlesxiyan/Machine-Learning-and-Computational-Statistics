{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02eb922",
   "metadata": {},
   "source": [
    "### Homework 3\n",
    "##### Charles Yan, xy2985"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe82aba",
   "metadata": {},
   "source": [
    "#### Q1 The classical linear regression model\n",
    "\n",
    "##### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1b9ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Frobenius Norm of P - P is 4.09e-16\n",
      "=============================================\n",
      "Frobenius Norm of P^2 - P is 6.87e-16\n",
      "=============================================\n",
      "Frobenius Norm of X^T(y - Py) is 3.23e-15\n",
      "=============================================\n",
      "Rank of P is 5.00e+00 v.s. p = 5.\n"
     ]
    }
   ],
   "source": [
    "### (b)\n",
    "import numpy as np\n",
    "np.random.seed(99)\n",
    "\n",
    "n, p = 50, 5\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "y = np.random.normal(0, 1, (n, 1))\n",
    "P = X @ np.linalg.solve(X.T @ X, X.T)\n",
    "\n",
    "### Tets\n",
    "print('=' * 45)\n",
    "print(f'Frobenius Norm of P - P is {np.linalg.norm(P - P.T):.2e}')\n",
    "print('=' * 45)\n",
    "print(f'Frobenius Norm of P^2 - P is {np.linalg.norm(P @ P - P):.2e}')\n",
    "print('=' * 45)\n",
    "print(f'Frobenius Norm of X^T(y - Py) is {np.linalg.norm(X.T @ (y - P @ y)):.2e}')\n",
    "print('=' * 45)\n",
    "print(f'Rank of P is {np.linalg.matrix_rank(P):.2e} v.s. p = {p}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f0cd6",
   "metadata": {},
   "source": [
    "##### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c21d5",
   "metadata": {},
   "source": [
    "##### (d) Proof\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc1e5d",
   "metadata": {},
   "source": [
    "##### (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (f)\n",
    "np.random.seed(99)\n",
    "n, p, B = 200, 3, 500\n",
    "beta = np.array([0.4, 0.5, 0.6])\n",
    "std = 1\n",
    "\n",
    "### (i)\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "A = np.linalg.inv(X.T @ X)\n",
    "varBetaHat = std ** 2 * A\n",
    "\n",
    "betaHatI = np.zeros((B, p))\n",
    "for b in range(B):\n",
    "    y = X @ beta + np.random.normal(0, 1, n) * std\n",
    "    betaHatI[b] = A @ X.T @ y\n",
    "empVarI = np.cov(betaHatI, rowvar=False)\n",
    "print('=' * 45)\n",
    "print(f'Emp Var - Real Var: {np.linalg.norm(empVarI - varBetaHat):.4e}')\n",
    "print('=' * 45)\n",
    "\n",
    "### (ii)\n",
    "betaHatII = np.zeros((B, p))\n",
    "for b in range(B):\n",
    "    XRand = np.random.normal(0, 1, (n, p))\n",
    "    y = XRand @ beta + np.random.normal(0, 1, n) * std\n",
    "    betaHatII[b] = np.linalg.solve(XRand.T @ XRand, XRand.T @ y)\n",
    "empVarII = np.cov(betaHatII, rowvar=False)\n",
    "print('=' * 45)\n",
    "print(f'Variance of each beta components (i): {np.diag(empVarI)}')\n",
    "print('=' * 45)\n",
    "print(f'Variance of each beta components (ii): {np.diag(empVarII)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa24cacd",
   "metadata": {},
   "source": [
    "##### Q2 Finite-sample properties of OLS\n",
    "\n",
    "##### (a) Assumptions\n",
    "##### (i) Linearity $\\bm{Y} = \\bm{X\\beta} + \\bm{\\epsilon}$.\n",
    "##### (ii) Strict Exogeneity $E[\\bm{\\epsilon\\vert\\bm{X}}] = \\bm{0}$.\n",
    "##### (iii) No Multicolinearity $P(Rank(\\bm{X}) = p) = 1$.\n",
    "##### (iv) Spherical Errors $Var(\\bm{\\epsilon}\\vert\\bm{X}) = \\sigma^{2}\\bm{I}_{n}$.\n",
    "##### (v) Normality $\\bm{\\epsilon}\\vert\\bm{X} \\sim N(\\bm{0}, \\sigma^{2}\\bm{I}_{n})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (b)\n",
    "np.random.seed(99)\n",
    "n, p, var, B = 80, 5, 2, 5000\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "beta = np.random.normal(0, 1, p)\n",
    "\n",
    "s2Bias = np.zeros(B)\n",
    "s2Unbias = np.zeros(B)\n",
    "\n",
    "for b in range(B):\n",
    "    eps = np.random.normal(0, np.sqrt(var), n)\n",
    "    y = X @ beta + eps\n",
    "    betaHat = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "    r = y - X @ betaHat\n",
    "    s2Unbias[b] = r @ r / (n - p)\n",
    "    s2Bias[b] = r @ r / n\n",
    "\n",
    "print('=' * 45)\n",
    "print(f'Empirical mean of s2Unbias: {s2Unbias.mean():.4e}, Real Variance: {var:.4e}')\n",
    "print('=' * 45)\n",
    "print(f'Empirical mean of s2Bias: {s2Bias.mean():.4e}, Real Variance: {var:.4e}')\n",
    "print('=' * 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b3ac6",
   "metadata": {},
   "source": [
    "##### As is shown in (a) that $\\dfrac{\\bm{r}^{\\top}\\bm{r}}{n - p}$ is an unbiased estimator of $\\bm{\\sigma}^{2}$. An adjustment on the denominator can make the statistic biased, say $\\dfrac{\\bm{r}^{\\top}\\bm{r}}{n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f56a0",
   "metadata": {},
   "source": [
    "##### (c)\n",
    "##### (i) Linearity $\\bm{Y} = \\bm{X\\beta} + \\bm{\\epsilon}$.\n",
    "##### (ii) Strict Exogeneity $E[\\bm{\\epsilon\\vert\\bm{X}}] = \\bm{0}$.\n",
    "##### (iii) No Multicolinearity $P(Rank(\\bm{X}) = p) = 1$.\n",
    "##### (iv) Spherical Errors $Var(\\bm{\\epsilon}\\vert\\bm{X}) = \\sigma^{2}\\bm{I}_{n}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b0243",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c27b663",
   "metadata": {},
   "source": [
    "##### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23811a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### (e)\n",
    "### Note: n, p, var, X, beta are the same as (b)\n",
    "np.random.seed(99)\n",
    "BList = [100, 500, 1000, 5000]\n",
    "\n",
    "for B in BList:\n",
    "    betaHats = np.zeros((B, p))\n",
    "    rs = np.zeros((B, n))\n",
    "\n",
    "    for b in range(B):\n",
    "        eps = np.random.normal(0, np.sqrt(var), n)\n",
    "        y = X @ beta + eps\n",
    "        betaHats[b] = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "        rs[b] = y - X @ betaHats[b]\n",
    "\n",
    "    betaHatMean = betaHats.mean(axis=0)\n",
    "    rMean = rs.mean(axis=0)\n",
    "    CHat = (betaHats - betaHatMean).T @ (rs - rMean) / (B - 1)\n",
    "\n",
    "    print('=' * 45)\n",
    "    print(f'B = {B}: Frobenius Norm of CHat is {np.linalg.norm(CHat):.4e}')\n",
    "    print('=' * 45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a9199",
   "metadata": {},
   "source": [
    "##### As shown above in the numerical outcome, the Frobenius norm of $\\hat{\\bm{C}}$ is shrinking as B goes larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f624759",
   "metadata": {},
   "source": [
    "##### Q3 Hypothesis Testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed3b3e",
   "metadata": {},
   "source": [
    "(a)\n",
    "\n",
    "(i) In the Linear Model $y = X\\beta + \\varepsilon$, we want to test the null hypothesis\n",
    "\n",
    "$$H_0: R\\beta = d$$\n",
    "\n",
    "where $R \\in \\mathbb{R}^{d \\times p}$ with rank d, and $d \\in \\mathbb{R}^d$ are known.\n",
    "\n",
    "(ii) We define F-statistic\n",
    "\n",
    "$$F = \\frac{(R\\hat{\\beta} - d)^{\\top}[R(X^{\\top}X)^{-1}R^{\\top}]^{-1}(R\\hat{\\beta} - d) / d}{s^2}$$\n",
    "\n",
    "where $s^2 = r^{\\top}r/(n-p)$. Under the classical linear regression assumptions (I) - (V) (as shown in Q2(a)), the F-statistic follows an F-distribution with (d, n - p) degrees of freedom under the null hypothesis, that is, $F|X, H_0 \\sim F(d, n-p)$. \n",
    "\n",
    "Given a confidence level $\\alpha\\in [0, 1]$, if $F > F_{d,n-p,\\alpha}$, we reject $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec2e70",
   "metadata": {},
   "source": [
    "(b)\n",
    "\n",
    "Under $H_0: \\beta_2 = \\cdots = \\beta_p = 0$. We construct such a matrix $\\bm{R}$\n",
    "\n",
    "$$\\bm{R} = [\\bm{0}_{p-1} \\mid \\bm{I}_{p-1}] \\in \\mathbb{R}^{(p-1) \\times p},$$\n",
    "\n",
    "and $\\bm{d} = \\bm{0}_{p-1} \\in \\mathbb{R}^{p-1}$.\n",
    "\n",
    "Thus, for $\\bm{\\beta} = [\\beta_1, \\beta_2, \\cdots, \\beta_p]$, \n",
    "\n",
    "$H_0: \\beta_2 = \\cdots = \\beta_p = 0$ is equivalent to $H_0: \\bm{R}\\bm{\\beta} = \\bm{d}$\n",
    "\n",
    "Based on the theory in (a):\n",
    "\n",
    "$$F = \\frac{(\\bm{R}\\hat{\\bm{\\beta}})^{\\top}[\\bm{R}(\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{R}^{\\top}]^{-1}(\\bm{R}\\hat{\\bm{\\beta}}) / (p-1)}{s^2} \\sim F(p-1, n-p)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d062770",
   "metadata": {},
   "source": [
    "(c)\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "(i) Linearity $\\bm{Y} = \\bm{X\\beta} + \\bm{\\epsilon}$.\n",
    "\n",
    "(ii) Strict Exogeneity $E[\\bm{\\epsilon\\vert\\bm{X}}] = \\bm{0}$.\n",
    "\n",
    "(iii) No Multicolinearity $P(Rank(\\bm{X}) = p) = 1$.\n",
    "\n",
    "(iv) Spherical Errors $Var(\\bm{\\epsilon}\\vert\\bm{X}) = \\sigma^{2}\\bm{I}_{n}$.\n",
    "\n",
    "(v) Normality $\\bm{\\epsilon}\\vert\\bm{X} \\sim N(\\bm{0}, \\sigma^{2}\\bm{I}_{n})$.\n",
    "\n",
    "Proof:\n",
    "\n",
    "Since $\\bm{\\epsilon}\\vert\\bm{X}\\sim N(\\bm{0}, \\sigma^2\\bm{I}_n)$ based on assumption (v), we have\n",
    "\n",
    "$$\\begin{aligned}\\hat{\\bm{\\beta}}\\vert\\bm{X} &= (\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top}\\bm{y}\\\\ &= \\bm{\\beta} + (\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top}\\bm{\\epsilon}\\\\ &\\sim N(\\bm{\\beta}, \\sigma^2(\\bm{X}^{\\top}\\bm{X})^{-1})\\end{aligned}$$\n",
    "\n",
    "Under $H_0$, we have $\\bm{R\\beta} - \\bm{d} = \\bm{0}$.\n",
    "\n",
    "Hence, we have \n",
    "\n",
    "$$\\begin{aligned}(\\bm{R}\\hat{\\bm{\\beta}} - \\bm{d})\\vert\\bm{X} \\sim N(\\bm{0}, \\sigma^2 \\bm{R}(\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{R}^{\\top})\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8af3e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\bm{\\beta}} &= (\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top}\\bm{y} \\\n",
    "&= (\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top}(\\bm{X\\beta} + \\bm{\\epsilon}) \\\n",
    "&= \\bm{\\beta} + (\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top}\\bm{\\epsilon}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Var}[\\hat{\\bm{\\beta}}|\\bm{X}] &= E[(\\hat{\\bm{\\beta}} - \\bm{\\beta})(\\hat{\\bm{\\beta}} - \\bm{\\beta})^{\\top}|\\bm{X}] \\\n",
    "&= E[(\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top}\\bm{\\epsilon},\\bm{\\epsilon}^{\\top}\\bm{X}(\\bm{X}^{\\top}\\bm{X})^{-1}|\\bm{X}] \\\n",
    "&= (\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top},E[\\bm{\\epsilon}\\bm{\\epsilon}^{\\top}|\\bm{X}],\\bm{X}(\\bm{X}^{\\top}\\bm{X})^{-1} \\\n",
    "&= (\\bm{X}^{\\top}\\bm{X})^{-1}\\bm{X}^{\\top},\\sigma^2\\bm{I}_n,\\bm{X}(\\bm{X}^{\\top}\\bm{X})^{-1} \\\n",
    "&= \\sigma^2(\\bm{X}^{\\top}\\bm{X})^{-1}(\\bm{X}^{\\top}\\bm{X})(\\bm{X}^{\\top}\\bm{X})^{-1} \\\n",
    "&= \\sigma^2(\\bm{X}^{\\top}\\bm{X})^{-1} \\quad \\blacksquare\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
